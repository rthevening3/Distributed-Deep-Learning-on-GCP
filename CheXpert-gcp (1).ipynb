{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Distributed Deep Learning Pipeline on GCP"]},{"cell_type":"markdown","metadata":{},"source":["Install libraries"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch\n","  Downloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","     |███████████████████████████     | 742.7 MB 108.4 MB/s eta 0:00:02███████████████████████      | 714.2 MB 108.4 MB/s eta 0:00:02"]},{"name":"stderr","output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]},{"name":"stdout","output_type":"stream","text":["     |████████████████████████████████| 881.9 MB 1.8 kB/s              \n","\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch) (3.10.0.2)\n","Installing collected packages: torch\n","Successfully installed torch-1.10.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Collecting torchvision\n","  Downloading torchvision-0.11.1-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n","     |████████████████████████████████| 23.3 MB 4.9 MB/s            \n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from torchvision) (1.19.5)\n","Requirement already satisfied: torch==1.10.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from torchvision) (1.10.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from torchvision) (8.3.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n","Installing collected packages: torchvision\n","Successfully installed torchvision-0.11.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Collecting cmake\n","  Downloading cmake-3.22.1-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.7 MB)\n","     |████████████████████████████████| 22.7 MB 4.8 MB/s            \n","\u001b[?25hInstalling collected packages: cmake\n","Successfully installed cmake-3.22.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Collecting horovod[pytorch,spark]\n","  Downloading horovod-0.23.0.tar.gz (3.4 MB)\n","     |████████████████████████████████| 3.4 MB 5.0 MB/s            \n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: cloudpickle in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (2.0.0)\n","Requirement already satisfied: psutil in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (5.8.0)\n","Requirement already satisfied: pyyaml in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (6.0)\n","Requirement already satisfied: cffi>=1.4.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (1.14.6)\n","Requirement already satisfied: torch in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (1.10.0)\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-1.5.5-py3-none-any.whl (525 kB)\n","     |████████████████████████████████| 525 kB 97.6 MB/s            \n","\u001b[?25hCollecting h5py<3\n","  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n","     |████████████████████████████████| 2.9 MB 84.4 MB/s            \n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (1.19.5)\n","Collecting petastorm>=0.11.0\n","  Downloading petastorm-0.11.3-py2.py3-none-any.whl (283 kB)\n","     |████████████████████████████████| 283 kB 72.6 MB/s            \n","\u001b[?25hRequirement already satisfied: pyarrow>=0.15.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (2.0.0)\n","Requirement already satisfied: fsspec in /opt/conda/miniconda3/lib/python3.8/site-packages (from horovod[pytorch,spark]) (0.9.0)\n","Requirement already satisfied: pyspark>=3.0.0 in /usr/lib/spark/python (from horovod[pytorch,spark]) (3.1.2)\n","Requirement already satisfied: pycparser in /opt/conda/miniconda3/lib/python3.8/site-packages (from cffi>=1.4.0->horovod[pytorch,spark]) (2.20)\n","Requirement already satisfied: six in /opt/conda/miniconda3/lib/python3.8/site-packages (from h5py<3->horovod[pytorch,spark]) (1.16.0)\n","Requirement already satisfied: future>=0.10.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from petastorm>=0.11.0->horovod[pytorch,spark]) (0.18.2)\n","Requirement already satisfied: packaging>=15.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from petastorm>=0.11.0->horovod[pytorch,spark]) (21.0)\n","Collecting dill>=0.2.1\n","  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n","     |████████████████████████████████| 86 kB 8.4 MB/s             \n","\u001b[?25hRequirement already satisfied: pandas>=0.19.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from petastorm>=0.11.0->horovod[pytorch,spark]) (1.2.5)\n","Collecting diskcache>=3.0.0\n","  Downloading diskcache-5.3.0-py3-none-any.whl (44 kB)\n","     |████████████████████████████████| 44 kB 4.7 MB/s             \n","\u001b[?25hRequirement already satisfied: pyzmq>=14.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from petastorm>=0.11.0->horovod[pytorch,spark]) (22.3.0)\n","Requirement already satisfied: py4j==0.10.9 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark>=3.0.0->horovod[pytorch,spark]) (0.10.9)\n","Requirement already satisfied: tqdm>=4.41.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pytorch_lightning->horovod[pytorch,spark]) (4.62.3)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","     |████████████████████████████████| 132 kB 102.0 MB/s            \n","\u001b[?25hCollecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n","     |████████████████████████████████| 332 kB 92.2 MB/s            \n","\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/miniconda3/lib/python3.8/site-packages (from pytorch_lightning->horovod[pytorch,spark]) (3.10.0.2)\n","Collecting pyDeprecate==0.3.1\n","  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n","Collecting tensorboard>=2.2.0\n","  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n","     |████████████████████████████████| 5.8 MB 76.4 MB/s            \n","\u001b[?25hRequirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (2.25.1)\n","Requirement already satisfied: aiohttp in /opt/conda/miniconda3/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (3.7.4.post0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from packaging>=15.0->petastorm>=0.11.0->horovod[pytorch,spark]) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas>=0.19.0->petastorm>=0.11.0->horovod[pytorch,spark]) (2.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas>=0.19.0->petastorm>=0.11.0->horovod[pytorch,spark]) (2021.3)\n","Collecting tensorboard-plugin-wit>=1.6.0\n","  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n","     |████████████████████████████████| 781 kB 71.9 MB/s            \n","\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (58.2.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (1.41.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (3.3.4)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","     |████████████████████████████████| 4.9 MB 74.5 MB/s            \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (3.17.3)\n","Collecting werkzeug>=0.11.15\n","  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n","     |████████████████████████████████| 288 kB 89.0 MB/s            \n","\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (0.4.6)\n","Requirement already satisfied: wheel>=0.26 in /opt/conda/miniconda3/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (0.37.0)\n","Collecting absl-py>=0.4\n","  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n","     |████████████████████████████████| 126 kB 86.5 MB/s            \n","\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (0.2.7)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (1.3.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (1.25.11)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (2.10)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (5.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (21.2.0)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (3.0.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->horovod[pytorch,spark]) (1.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning->horovod[pytorch,spark]) (3.1.1)\n","Building wheels for collected packages: horovod\n","  Building wheel for horovod (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for horovod: filename=horovod-0.23.0-cp38-cp38-linux_x86_64.whl size=11162850 sha256=f71497e5e540c8c4933e3b73cb991a91d8d304b3596332792e95fa24e028ef2c\n","  Stored in directory: /root/.cache/pip/wheels/cc/58/a9/a02ee995504a06826aaf303673f96cf93c460fdf1e76894862\n","Successfully built horovod\n","Installing collected packages: werkzeug, tensorboard-plugin-wit, tensorboard-data-server, fsspec, absl-py, torchmetrics, tensorboard, pyDeprecate, diskcache, dill, pytorch-lightning, petastorm, horovod, h5py\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 0.9.0\n","    Uninstalling fsspec-0.9.0:\n","      Successfully uninstalled fsspec-0.9.0\n","Successfully installed absl-py-1.0.0 dill-0.3.4 diskcache-5.3.0 fsspec-2021.11.1 h5py-2.10.0 horovod-0.23.0 petastorm-0.11.3 pyDeprecate-0.3.1 pytorch-lightning-1.5.5 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 torchmetrics-0.6.1 werkzeug-2.0.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip install torch\n","!pip install torchvision\n","!pip install cmake\n","!pip install horovod[pytorch,spark]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","import subprocess\n","import sys\n","import numpy as np\n","\n","import pyspark\n","import pyspark.sql.types as T\n","from pyspark import SparkConf\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import udf\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import horovod.spark.torch as hvd\n","from horovod.spark.common.backend import SparkBackend\n","from horovod.spark.common.store import Store"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]}],"source":["conf = SparkConf().setAppName('pytorch_spark_CheXpert').set('spark.sql.shuffle.partitions', '2')\n","spark = SparkSession.builder.config(conf=conf).getOrCreate()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- image: struct (nullable = true)\n"," |    |-- origin: string (nullable = true)\n"," |    |-- height: integer (nullable = true)\n"," |    |-- width: integer (nullable = true)\n"," |    |-- nChannels: integer (nullable = true)\n"," |    |-- mode: integer (nullable = true)\n"," |    |-- data: binary (nullable = true)\n","\n","CPU times: user 238 ms, sys: 73.1 ms, total: 311 ms\n","Wall time: 1min 19s\n"]}],"source":["%%time\n","patientImagedf = spark.read.format(\"image\").option(\"dropInvalid\", True).load(\"gs://chexpertcse6250fall2021/CheXpert-v1.0-small/train/*/*\")\n","patientImagedf.printSchema()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["10001"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["patientImagedf.count()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sb\n","import cv2\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["sparkLabelsdf = spark.read.option('header', True).\\\n","csv('gs://chexpertcse6250fall2021/CheXpert-v1.0-small/train_mod.csv')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- No Finding: string (nullable = true)\n"," |-- Enlarged Cardiomediastinum: string (nullable = true)\n"," |-- Cardiomegaly: string (nullable = true)\n"," |-- Lung Opacity: string (nullable = true)\n"," |-- Lung Lesion: string (nullable = true)\n"," |-- Edema: string (nullable = true)\n"," |-- Consolidation: string (nullable = true)\n"," |-- Pneumonia: string (nullable = true)\n"," |-- Atelectasis: string (nullable = true)\n"," |-- Pneumothorax: string (nullable = true)\n"," |-- Pleural Effusion: string (nullable = true)\n"," |-- Pleural Other: string (nullable = true)\n"," |-- Fracture: string (nullable = true)\n"," |-- Support Devices: string (nullable = true)\n","\n"]}],"source":["sparkLabelsdf.printSchema()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- NoFinding: string (nullable = true)\n"," |-- EnlargedCardiomediastinum: string (nullable = true)\n"," |-- Cardiomegaly: string (nullable = true)\n"," |-- LungOpacity: string (nullable = true)\n"," |-- LungLesion: string (nullable = true)\n"," |-- Edema: string (nullable = true)\n"," |-- Consolidation: string (nullable = true)\n"," |-- Pneumonia: string (nullable = true)\n"," |-- Atelectasis: string (nullable = true)\n"," |-- Pneumothorax: string (nullable = true)\n"," |-- PleuralEffusion: string (nullable = true)\n"," |-- PleuralOther: string (nullable = true)\n"," |-- Fracture: string (nullable = true)\n"," |-- SupportDevices: string (nullable = true)\n","\n"]}],"source":["# Change Column Names (to remove spaces)\n","sparkLabelsdf = sparkLabelsdf.withColumnRenamed(\"No Finding\",\"NoFinding\") \\\n",".withColumnRenamed('Enlarged Cardiomediastinum','EnlargedCardiomediastinum')\\\n",".withColumnRenamed('Lung Opacity','LungOpacity')\\\n",".withColumnRenamed('Lung Lesion','LungLesion')\\\n",".withColumnRenamed('Pleural Effusion','PleuralEffusion')\\\n",".withColumnRenamed('Pleural Other','PleuralOther')\\\n",".withColumnRenamed('Support Devices','SupportDevices')\n","\n","sparkLabelsdf.printSchema()\n"]},{"cell_type":"markdown","metadata":{},"source":["Label standardization:\n"," - Findings with a '-1' label which implies uncertainty is replaced with 0"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import *\n","\n","\n","standardLabels = sparkLabelsdf\\\n",".withColumn('NoFinding_mod', when(sparkLabelsdf.NoFinding == 1, 1.0)\n","            .when(sparkLabelsdf.NoFinding == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('EnlargedCardiomediastinum_mod', when(sparkLabelsdf.EnlargedCardiomediastinum == '1', 1.0)\n","            .when(sparkLabelsdf.EnlargedCardiomediastinum == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Cardiomegaly_mod', when(sparkLabelsdf.Cardiomegaly == 1, 1.0)\n","            .when(sparkLabelsdf.Cardiomegaly == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('LungOpacity_mod', when(sparkLabelsdf.LungOpacity == 1, 1.0)\n","            .when(sparkLabelsdf.LungOpacity == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('LungLesion_mod', when(sparkLabelsdf.LungLesion == 1, 1.0)\n","            .when(sparkLabelsdf.LungLesion == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Edema_mod', when(sparkLabelsdf.Edema == 1, 1.0)\n","            .when(sparkLabelsdf.Edema == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Consolidation_mod', when(sparkLabelsdf.Consolidation == 1, 1.0)\n","            .when(sparkLabelsdf.Consolidation == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Pneumonia_mod', when(sparkLabelsdf.Pneumonia == 1, 1.0)\n","            .when(sparkLabelsdf.Pneumonia == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Atelectasis_mod', when(sparkLabelsdf.Atelectasis == '1', 1.0)\n","            .when(sparkLabelsdf.Atelectasis == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Pneumothorax_mod', when(sparkLabelsdf.Pneumothorax == '1', 1.0)\n","            .when(sparkLabelsdf.Pneumothorax ==-1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('PleuralEffusion_mod', when(sparkLabelsdf.PleuralEffusion == '1', 1.0)\n","            .when(sparkLabelsdf.PleuralEffusion == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('PleuralOther_mod', when(sparkLabelsdf.PleuralOther == '1', 1.0)\n","            .when(sparkLabelsdf.PleuralOther == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Fracture_mod', when(sparkLabelsdf.Fracture == 1, 1.0)\n","            .when(sparkLabelsdf.Fracture == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('SupportDevices_mod', when(sparkLabelsdf.SupportDevices == 1, 1.0)\n","            .when(sparkLabelsdf.SupportDevices == -1, 0.0)\n","            .otherwise(0.0)) \\\n",".drop('NoFinding', 'LungLesion', 'EnlargedCardiomediastinum','Edema', 'Consolidation', 'Pneumonia',\n","      'Atelectasis', 'Pneumothorax', 'PleuralEffusion', 'PleuralOther',\n","     'Fracture', 'SupportDevices', 'Cardiomegaly', 'LungOpacity')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- NoFinding_mod: double (nullable = false)\n"," |-- EnlargedCardiomediastinum_mod: double (nullable = false)\n"," |-- Cardiomegaly_mod: double (nullable = false)\n"," |-- LungOpacity_mod: double (nullable = false)\n"," |-- LungLesion_mod: double (nullable = false)\n"," |-- Edema_mod: double (nullable = false)\n"," |-- Consolidation_mod: double (nullable = false)\n"," |-- Pneumonia_mod: double (nullable = false)\n"," |-- Atelectasis_mod: double (nullable = false)\n"," |-- Pneumothorax_mod: double (nullable = false)\n"," |-- PleuralEffusion_mod: double (nullable = false)\n"," |-- PleuralOther_mod: double (nullable = false)\n"," |-- Fracture_mod: double (nullable = false)\n"," |-- SupportDevices_mod: double (nullable = false)\n","\n"]}],"source":["standardLabels.printSchema()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import regexp_replace, col\n","\n","patientImagedfMod = patientImagedf.withColumn('pathgcp', regexp_replace('image.origin', 'gs://chexpertcse6250fall2021/',''))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["patientImagedfMod = patientImagedfMod.select('image.origin', 'image.height', 'image.width', 'image.nChannels', 'image.mode', 'image.data', 'pathgcp')"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- origin: string (nullable = true)\n"," |-- height: integer (nullable = true)\n"," |-- width: integer (nullable = true)\n"," |-- nChannels: integer (nullable = true)\n"," |-- mode: integer (nullable = true)\n"," |-- data: binary (nullable = true)\n"," |-- pathgcp: string (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+------+-----+---------+----+--------------------+--------------------+\n","|              origin|height|width|nChannels|mode|                data|             pathgcp|\n","+--------------------+------+-----+---------+----+--------------------+--------------------+\n","|gs://chexpertcse6...|   320|  652|        1|   0|[FD FE FF FF FD F...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  425|        1|   0|[08 0B 0D 0D 0D 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  422|        1|   0|[02 0A 05 03 06 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  413|        1|   0|[25 38 39 31 34 3...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   452|  320|        1|   0|[0E 16 16 0C 13 1...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[06 05 03 02 02 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  421|        1|   0|[02 02 03 03 03 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  423|        1|   0|[11 0F 10 18 1E 2...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[02 02 02 03 05 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  406|        1|   0|[04 04 04 04 04 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[06 06 06 06 06 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[16 10 15 12 19 1...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  410|        1|   0|[2D 2A 3C 2A 3D 4...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  441|        1|   0|[3D 47 7F 87 83 7...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[05 05 05 05 05 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[03 04 06 05 04 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[09 08 07 07 07 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[0A 0A 0C 0C 0C 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[3E 13 17 16 05 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[FF FF FF FF FF F...|CheXpert-v1.0-sma...|\n","+--------------------+------+-----+---------+----+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["patientImagedfMod.printSchema()\n","patientImagedfMod.show()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- origin: string (nullable = true)\n"," |-- height: integer (nullable = true)\n"," |-- width: integer (nullable = true)\n"," |-- nChannels: integer (nullable = true)\n"," |-- mode: integer (nullable = true)\n"," |-- data: binary (nullable = true)\n"," |-- pathgcp: string (nullable = true)\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- NoFinding_mod: double (nullable = false)\n"," |-- EnlargedCardiomediastinum_mod: double (nullable = false)\n"," |-- Cardiomegaly_mod: double (nullable = false)\n"," |-- LungOpacity_mod: double (nullable = false)\n"," |-- LungLesion_mod: double (nullable = false)\n"," |-- Edema_mod: double (nullable = false)\n"," |-- Consolidation_mod: double (nullable = false)\n"," |-- Pneumonia_mod: double (nullable = false)\n"," |-- Atelectasis_mod: double (nullable = false)\n"," |-- Pneumothorax_mod: double (nullable = false)\n"," |-- PleuralEffusion_mod: double (nullable = false)\n"," |-- PleuralOther_mod: double (nullable = false)\n"," |-- Fracture_mod: double (nullable = false)\n"," |-- SupportDevices_mod: double (nullable = false)\n","\n"]}],"source":["joineddf = patientImagedfMod.join(standardLabels, patientImagedfMod.pathgcp == standardLabels.Path, 'inner')\n","joineddf.printSchema()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["7826"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["joineddf.count()"]},{"cell_type":"markdown","metadata":{},"source":["Keep only the frontal images"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 12:=====================================================>(311 + 2) / 313]\r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 227 ms, sys: 75.9 ms, total: 303 ms\n","Wall time: 2min 3s\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["7826"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","joineddf = joineddf.filter(joineddf.pathgcp.contains('frontal'))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- data: binary (nullable = true)\n"," |-- height: integer (nullable = true)\n"," |-- width: integer (nullable = true)\n"," |-- NoFinding_mod: double (nullable = false)\n"," |-- EnlargedCardiomediastinum_mod: double (nullable = false)\n"," |-- Cardiomegaly_mod: double (nullable = false)\n"," |-- LungOpacity_mod: double (nullable = false)\n"," |-- LungLesion_mod: double (nullable = false)\n"," |-- Edema_mod: double (nullable = false)\n"," |-- Consolidation_mod: double (nullable = false)\n"," |-- Pneumonia_mod: double (nullable = false)\n"," |-- Atelectasis_mod: double (nullable = false)\n"," |-- Pneumothorax_mod: double (nullable = false)\n"," |-- PleuralEffusion_mod: double (nullable = false)\n"," |-- PleuralOther_mod: double (nullable = false)\n"," |-- Fracture_mod: double (nullable = false)\n"," |-- SupportDevices_mod: double (nullable = false)\n","\n"]}],"source":["\n","trainingdf = joineddf.select('data', 'height', 'width', 'NoFinding_mod', 'EnlargedCardiomediastinum_mod',\n","                             'Cardiomegaly_mod', 'LungOpacity_mod', 'LungLesion_mod', 'Edema_mod',\n","                             'Consolidation_mod', 'Pneumonia_mod', 'Atelectasis_mod', 'Pneumothorax_mod', \n","                             'PleuralEffusion_mod', 'PleuralOther_mod','Fracture_mod', 'SupportDevices_mod')\n","\n","\n","trainingdf.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["Making a vector column out of the 14 labels"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","\n","assembler = VectorAssembler(\n","    inputCols=[\"NoFinding_mod\", \"EnlargedCardiomediastinum_mod\", \"Cardiomegaly_mod\",\"LungOpacity_mod\", \n","              \"LungLesion_mod\", \"Edema_mod\", \"Consolidation_mod\",\"Pneumonia_mod\",\"Atelectasis_mod\",\"PleuralEffusion_mod\",\n","              \"PleuralOther_mod\",\"Fracture_mod\",\"SupportDevices_mod\"],\n","    outputCol=\"labels\")\n","\n","output = assembler.transform(joineddf)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["21/12/13 00:57:35 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","[Stage 16:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|                Path|              labels|\n","+--------------------+--------------------+\n","|CheXpert-v1.0-sma...|          (13,[],[])|\n","|CheXpert-v1.0-sma...|(13,[3,5,12],[1.0...|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[6],[1.0])|\n","|CheXpert-v1.0-sma...|     (13,[12],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[5],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[5,6],[1.0,1.0])|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[0,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|      (13,[5],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[6],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[5,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|     (13,[12],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[0,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|(13,[5,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|      (13,[0],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[2,3,12],[1.0...|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[6],[1.0])|\n","+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["output.select(col('Path'), col('labels')).show()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import col\n","from petastorm.spark import SparkDatasetConverter, make_spark_converter\n","import io\n","import numpy as np\n","import torch\n","import torchvision\n","from PIL import Image\n","from functools import partial \n","from petastorm import TransformSpec\n","from torchvision import transforms \n","# from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n","import horovod.torch as hvd\n","# from sparkdl import HorovodRunner"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["df_train, df_val = trainingdf.randomSplit([0.9, 0.1], seed=12345)\n","\n","# Make sure the number of partitions is at least the number of workers which is required for distributed training.\n","df_train = df_train.repartition(2)\n","df_val = df_val.repartition(2)"]},{"cell_type":"markdown","metadata":{},"source":["### Image data transformation"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def rawBytesToPIL(img_str, height, width):\n","    nparr = np.frombuffer(img_str, np.uint8).reshape(height, width, 1)\n","    img_np = cv2.cvtColor(nparr, cv2.COLOR_BGR2RGB)\n","    img = Image.fromarray(img_np)\n","    return img"]},{"cell_type":"markdown","metadata":{},"source":["Saving the standardized dataframe to a Parquet file"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 277 ms, sys: 50.8 ms, total: 328 ms\n","Wall time: 2min 24s\n"]}],"source":["%%time\n","df_train \\\n","    .coalesce(1) \\\n","    .write \\\n","    .mode('overwrite') \\\n","    .option('parquet.block.size', 1024*1024) \\\n","    .parquet('gs://chexpertcse6250fall2021/parquetCache4')"]},{"cell_type":"markdown","metadata":{},"source":["Create a spark converter instance for the train and validation dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Converting floating-point columns to float32\n","[Stage 34:=================================================>        (6 + 1) / 7]\r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 302 ms, sys: 110 ms, total: 411 ms\n","Wall time: 3min 6s\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["%%time\n","spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"gs://chexpertcse6250fall2021/parquetCache4\")\n","\n","converter_train = make_spark_converter(df_train)\n","converter_val = make_spark_converter(df_val)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train: 7027\n"]}],"source":["print(f\"train: {len(converter_train)}\")"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["<petastorm.spark.spark_dataset_converter.TorchDatasetContextManager at 0x7fa6d2340760>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["converter_train.make_torch_dataloader()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def metric_average(val, name):\n","    tensor = torch.tensor(val)\n","    avg_tensor = hvd.allreduce(tensor, name=name)\n","    return avg_tensor.item()\n","\n","def train_and_evaluate_hvd(lr=0.001):\n","    hvd.init()  # Initialize Horovod.\n","  \n","  # Horovod: pin GPU to local rank.\n","    if torch.cuda.is_available():\n","        torch.cuda.set_device(hvd.local_rank())\n","        device = torch.cuda.current_device()\n","    else:\n","        device = torch.device(\"cpu\")\n","  \n","    model = get_model(lr=lr)\n","    model = model.to(device)\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","  \n","  # Effective batch size in synchronous distributed training is scaled by the number of workers.\n","  # An increase in learning rate compensates for the increased batch size.\n","    optimizer = torch.optim.SGD(model.classifier[1].parameters(), lr=lr * hvd.size(), momentum=0.9)\n","  \n","  # Broadcast initial parameters so all workers start with the same parameters.\n","    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n","    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n","  \n","  # Wrap the optimizer with Horovod's DistributedOptimizer.\n","    optimizer_hvd = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n","\n","    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_hvd, step_size=7, gamma=0.1)\n","\n","    with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(is_train=True), \n","                                             cur_shard=hvd.rank(), shard_count=hvd.size(),\n","                                             batch_size=BATCH_SIZE) as train_dataloader, \\\n","       converter_val.make_torch_dataloader(transform_spec=get_transform_spec(is_train=False),\n","                                           cur_shard=hvd.rank(), shard_count=hvd.size(),\n","                                           batch_size=BATCH_SIZE) as val_dataloader:\n","    \n","        train_dataloader_iter = iter(train_dataloader)\n","        steps_per_epoch = len(converter_train) // (BATCH_SIZE * hvd.size())\n","    \n","        val_dataloader_iter = iter(val_dataloader)\n","        validation_steps = max(1, len(converter_val) // (BATCH_SIZE * hvd.size()))\n","    \n","        for epoch in range(NUM_EPOCHS):\n","            print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n","            print('-' * 10)\n","\n","            train_loss, train_acc = train_one_epoch(model, criterion, optimizer_hvd, exp_lr_scheduler, \n","                                              train_dataloader_iter, steps_per_epoch, epoch, \n","                                              device)\n","            val_loss, val_acc = evaluate(model, criterion, val_dataloader_iter, validation_steps,\n","                                   device, metric_agg_fn=metric_average)\n","\n","    return val_loss"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e74edd46504b4fd0abfa75ad596ac16f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/13.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["MobileNetV2(\n","  (features): Sequential(\n","    (0): ConvNormActivation(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","    (1): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (7): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (8): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (9): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (10): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (11): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (12): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (13): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (14): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (15): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (16): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (17): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (18): ConvNormActivation(\n","      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.2, inplace=False)\n","    (1): Linear(in_features=1280, out_features=1000, bias=True)\n","  )\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["torchvision.models.mobilenet_v2(pretrained=True)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def get_model(lr=0.001):\n","  # Load a MobileNetV2 model from torchvision\n","  model = torchvision.models.mobilenet_v2(pretrained=True)\n","  # Freeze parameters in the feature extraction layers\n","  for param in model.parameters():\n","    param.requires_grad = False\n","    \n","  # Add a new classifier layer for transfer learning\n","  num_ftrs = model.classifier[1].in_features\n","  # Parameters of newly constructed modules have requires_grad=True by default\n","  model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{},"source":["This line fails on GCP"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# from sparkdl import HorovodRunner"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["# !pip install keras\n","# !pip install tensorflow\n","# !pip install tensorframes"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["import horovod.spark.common._namedtuple_fix\n","\n","import copy\n","import io\n","import numbers\n","import time\n","\n","from pyspark import keyword_only\n","from pyspark.ml.param.shared import Param, Params, TypeConverters\n","from pyspark.ml.util import MLWritable, MLReadable\n","from pyspark.sql import SparkSession\n","\n","from horovod.runner.common.util import codec\n","from horovod.spark.common import util\n","from horovod.spark.common.estimator import HorovodEstimator, HorovodModel\n","from horovod.spark.common.params import EstimatorParams\n","from horovod.spark.common.serialization import \\\n","    HorovodParamsWriter, HorovodParamsReader\n","from horovod.spark.torch import remote\n","from horovod.spark.torch.util import deserialize_fn, serialize_fn, \\\n","    save_into_bio"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import horovod.spark.keras as hvd\n","from horovod.spark.common.backend import SparkBackend\n","from horovod.spark.common.store import Store\n","from horovod.tensorflow.keras.callbacks import BestModelCheckpoint"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting package metadata (current_repodata.json): done\n","Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n","Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n","Collecting package metadata (repodata.json): - "]}],"source":["!conda install tensorflow"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":4}
