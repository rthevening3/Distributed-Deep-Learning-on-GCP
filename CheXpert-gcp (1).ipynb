{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Distributed Deep Learning Pipeline on GCP"]},{"cell_type":"markdown","metadata":{},"source":["Install libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install torch\n","!pip install torchvision\n","!pip install cmake\n","!pip install horovod[pytorch,spark]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","import subprocess\n","import sys\n","import numpy as np\n","\n","import pyspark\n","import pyspark.sql.types as T\n","from pyspark import SparkConf\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import udf\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import horovod.spark.torch as hvd\n","from horovod.spark.common.backend import SparkBackend\n","from horovod.spark.common.store import Store"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]}],"source":["conf = SparkConf().setAppName('pytorch_spark_CheXpert').set('spark.sql.shuffle.partitions', '2')\n","spark = SparkSession.builder.config(conf=conf).getOrCreate()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- image: struct (nullable = true)\n"," |    |-- origin: string (nullable = true)\n"," |    |-- height: integer (nullable = true)\n"," |    |-- width: integer (nullable = true)\n"," |    |-- nChannels: integer (nullable = true)\n"," |    |-- mode: integer (nullable = true)\n"," |    |-- data: binary (nullable = true)\n","\n","CPU times: user 238 ms, sys: 73.1 ms, total: 311 ms\n","Wall time: 1min 19s\n"]}],"source":["%%time\n","patientImagedf = spark.read.format(\"image\").option(\"dropInvalid\", True).load(\"gs://chexpertcse6250fall2021/CheXpert-v1.0-small/train/*/*\")\n","patientImagedf.printSchema()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["10001"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["patientImagedf.count()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sb\n","import cv2\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["sparkLabelsdf = spark.read.option('header', True).\\\n","csv('gs://chexpertcse6250fall2021/CheXpert-v1.0-small/train_mod.csv')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- No Finding: string (nullable = true)\n"," |-- Enlarged Cardiomediastinum: string (nullable = true)\n"," |-- Cardiomegaly: string (nullable = true)\n"," |-- Lung Opacity: string (nullable = true)\n"," |-- Lung Lesion: string (nullable = true)\n"," |-- Edema: string (nullable = true)\n"," |-- Consolidation: string (nullable = true)\n"," |-- Pneumonia: string (nullable = true)\n"," |-- Atelectasis: string (nullable = true)\n"," |-- Pneumothorax: string (nullable = true)\n"," |-- Pleural Effusion: string (nullable = true)\n"," |-- Pleural Other: string (nullable = true)\n"," |-- Fracture: string (nullable = true)\n"," |-- Support Devices: string (nullable = true)\n","\n"]}],"source":["sparkLabelsdf.printSchema()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- NoFinding: string (nullable = true)\n"," |-- EnlargedCardiomediastinum: string (nullable = true)\n"," |-- Cardiomegaly: string (nullable = true)\n"," |-- LungOpacity: string (nullable = true)\n"," |-- LungLesion: string (nullable = true)\n"," |-- Edema: string (nullable = true)\n"," |-- Consolidation: string (nullable = true)\n"," |-- Pneumonia: string (nullable = true)\n"," |-- Atelectasis: string (nullable = true)\n"," |-- Pneumothorax: string (nullable = true)\n"," |-- PleuralEffusion: string (nullable = true)\n"," |-- PleuralOther: string (nullable = true)\n"," |-- Fracture: string (nullable = true)\n"," |-- SupportDevices: string (nullable = true)\n","\n"]}],"source":["# Change Column Names (to remove spaces)\n","sparkLabelsdf = sparkLabelsdf.withColumnRenamed(\"No Finding\",\"NoFinding\") \\\n",".withColumnRenamed('Enlarged Cardiomediastinum','EnlargedCardiomediastinum')\\\n",".withColumnRenamed('Lung Opacity','LungOpacity')\\\n",".withColumnRenamed('Lung Lesion','LungLesion')\\\n",".withColumnRenamed('Pleural Effusion','PleuralEffusion')\\\n",".withColumnRenamed('Pleural Other','PleuralOther')\\\n",".withColumnRenamed('Support Devices','SupportDevices')\n","\n","sparkLabelsdf.printSchema()\n"]},{"cell_type":"markdown","metadata":{},"source":["Label standardization:\n"," - Findings with a '-1' label which implies uncertainty is replaced with 0"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import *\n","\n","\n","standardLabels = sparkLabelsdf\\\n",".withColumn('NoFinding_mod', when(sparkLabelsdf.NoFinding == 1, 1.0)\n","            .when(sparkLabelsdf.NoFinding == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('EnlargedCardiomediastinum_mod', when(sparkLabelsdf.EnlargedCardiomediastinum == '1', 1.0)\n","            .when(sparkLabelsdf.EnlargedCardiomediastinum == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Cardiomegaly_mod', when(sparkLabelsdf.Cardiomegaly == 1, 1.0)\n","            .when(sparkLabelsdf.Cardiomegaly == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('LungOpacity_mod', when(sparkLabelsdf.LungOpacity == 1, 1.0)\n","            .when(sparkLabelsdf.LungOpacity == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('LungLesion_mod', when(sparkLabelsdf.LungLesion == 1, 1.0)\n","            .when(sparkLabelsdf.LungLesion == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Edema_mod', when(sparkLabelsdf.Edema == 1, 1.0)\n","            .when(sparkLabelsdf.Edema == '-1', 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Consolidation_mod', when(sparkLabelsdf.Consolidation == 1, 1.0)\n","            .when(sparkLabelsdf.Consolidation == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Pneumonia_mod', when(sparkLabelsdf.Pneumonia == 1, 1.0)\n","            .when(sparkLabelsdf.Pneumonia == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Atelectasis_mod', when(sparkLabelsdf.Atelectasis == '1', 1.0)\n","            .when(sparkLabelsdf.Atelectasis == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Pneumothorax_mod', when(sparkLabelsdf.Pneumothorax == '1', 1.0)\n","            .when(sparkLabelsdf.Pneumothorax ==-1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('PleuralEffusion_mod', when(sparkLabelsdf.PleuralEffusion == '1', 1.0)\n","            .when(sparkLabelsdf.PleuralEffusion == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('PleuralOther_mod', when(sparkLabelsdf.PleuralOther == '1', 1.0)\n","            .when(sparkLabelsdf.PleuralOther == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('Fracture_mod', when(sparkLabelsdf.Fracture == 1, 1.0)\n","            .when(sparkLabelsdf.Fracture == -1, 0.0)\n","            .otherwise(0.0))\\\n",".withColumn('SupportDevices_mod', when(sparkLabelsdf.SupportDevices == 1, 1.0)\n","            .when(sparkLabelsdf.SupportDevices == -1, 0.0)\n","            .otherwise(0.0)) \\\n",".drop('NoFinding', 'LungLesion', 'EnlargedCardiomediastinum','Edema', 'Consolidation', 'Pneumonia',\n","      'Atelectasis', 'Pneumothorax', 'PleuralEffusion', 'PleuralOther',\n","     'Fracture', 'SupportDevices', 'Cardiomegaly', 'LungOpacity')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- NoFinding_mod: double (nullable = false)\n"," |-- EnlargedCardiomediastinum_mod: double (nullable = false)\n"," |-- Cardiomegaly_mod: double (nullable = false)\n"," |-- LungOpacity_mod: double (nullable = false)\n"," |-- LungLesion_mod: double (nullable = false)\n"," |-- Edema_mod: double (nullable = false)\n"," |-- Consolidation_mod: double (nullable = false)\n"," |-- Pneumonia_mod: double (nullable = false)\n"," |-- Atelectasis_mod: double (nullable = false)\n"," |-- Pneumothorax_mod: double (nullable = false)\n"," |-- PleuralEffusion_mod: double (nullable = false)\n"," |-- PleuralOther_mod: double (nullable = false)\n"," |-- Fracture_mod: double (nullable = false)\n"," |-- SupportDevices_mod: double (nullable = false)\n","\n"]}],"source":["standardLabels.printSchema()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import regexp_replace, col\n","\n","patientImagedfMod = patientImagedf.withColumn('pathgcp', regexp_replace('image.origin', 'gs://chexpertcse6250fall2021/',''))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["patientImagedfMod = patientImagedfMod.select('image.origin', 'image.height', 'image.width', 'image.nChannels', 'image.mode', 'image.data', 'pathgcp')"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- origin: string (nullable = true)\n"," |-- height: integer (nullable = true)\n"," |-- width: integer (nullable = true)\n"," |-- nChannels: integer (nullable = true)\n"," |-- mode: integer (nullable = true)\n"," |-- data: binary (nullable = true)\n"," |-- pathgcp: string (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+------+-----+---------+----+--------------------+--------------------+\n","|              origin|height|width|nChannels|mode|                data|             pathgcp|\n","+--------------------+------+-----+---------+----+--------------------+--------------------+\n","|gs://chexpertcse6...|   320|  652|        1|   0|[FD FE FF FF FD F...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  425|        1|   0|[08 0B 0D 0D 0D 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  422|        1|   0|[02 0A 05 03 06 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  413|        1|   0|[25 38 39 31 34 3...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   452|  320|        1|   0|[0E 16 16 0C 13 1...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[06 05 03 02 02 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  421|        1|   0|[02 02 03 03 03 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  423|        1|   0|[11 0F 10 18 1E 2...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[02 02 02 03 05 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  406|        1|   0|[04 04 04 04 04 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[06 06 06 06 06 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[16 10 15 12 19 1...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  410|        1|   0|[2D 2A 3C 2A 3D 4...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  441|        1|   0|[3D 47 7F 87 83 7...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[05 05 05 05 05 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  389|        1|   0|[03 04 06 05 04 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[09 08 07 07 07 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[0A 0A 0C 0C 0C 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[3E 13 17 16 05 0...|CheXpert-v1.0-sma...|\n","|gs://chexpertcse6...|   320|  390|        1|   0|[FF FF FF FF FF F...|CheXpert-v1.0-sma...|\n","+--------------------+------+-----+---------+----+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["patientImagedfMod.printSchema()\n","patientImagedfMod.show()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- origin: string (nullable = true)\n"," |-- height: integer (nullable = true)\n"," |-- width: integer (nullable = true)\n"," |-- nChannels: integer (nullable = true)\n"," |-- mode: integer (nullable = true)\n"," |-- data: binary (nullable = true)\n"," |-- pathgcp: string (nullable = true)\n"," |-- Path: string (nullable = true)\n"," |-- Sex: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Frontal/Lateral: string (nullable = true)\n"," |-- AP/PA: string (nullable = true)\n"," |-- NoFinding_mod: double (nullable = false)\n"," |-- EnlargedCardiomediastinum_mod: double (nullable = false)\n"," |-- Cardiomegaly_mod: double (nullable = false)\n"," |-- LungOpacity_mod: double (nullable = false)\n"," |-- LungLesion_mod: double (nullable = false)\n"," |-- Edema_mod: double (nullable = false)\n"," |-- Consolidation_mod: double (nullable = false)\n"," |-- Pneumonia_mod: double (nullable = false)\n"," |-- Atelectasis_mod: double (nullable = false)\n"," |-- Pneumothorax_mod: double (nullable = false)\n"," |-- PleuralEffusion_mod: double (nullable = false)\n"," |-- PleuralOther_mod: double (nullable = false)\n"," |-- Fracture_mod: double (nullable = false)\n"," |-- SupportDevices_mod: double (nullable = false)\n","\n"]}],"source":["joineddf = patientImagedfMod.join(standardLabels, patientImagedfMod.pathgcp == standardLabels.Path, 'inner')\n","joineddf.printSchema()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["7826"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["joineddf.count()"]},{"cell_type":"markdown","metadata":{},"source":["Keep only the frontal images"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 12:=====================================================>(311 + 2) / 313]\r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 227 ms, sys: 75.9 ms, total: 303 ms\n","Wall time: 2min 3s\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["7826"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","joineddf = joineddf.filter(joineddf.pathgcp.contains('frontal'))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- data: binary (nullable = true)\n"," |-- height: integer (nullable = true)\n"," |-- width: integer (nullable = true)\n"," |-- NoFinding_mod: double (nullable = false)\n"," |-- EnlargedCardiomediastinum_mod: double (nullable = false)\n"," |-- Cardiomegaly_mod: double (nullable = false)\n"," |-- LungOpacity_mod: double (nullable = false)\n"," |-- LungLesion_mod: double (nullable = false)\n"," |-- Edema_mod: double (nullable = false)\n"," |-- Consolidation_mod: double (nullable = false)\n"," |-- Pneumonia_mod: double (nullable = false)\n"," |-- Atelectasis_mod: double (nullable = false)\n"," |-- Pneumothorax_mod: double (nullable = false)\n"," |-- PleuralEffusion_mod: double (nullable = false)\n"," |-- PleuralOther_mod: double (nullable = false)\n"," |-- Fracture_mod: double (nullable = false)\n"," |-- SupportDevices_mod: double (nullable = false)\n","\n"]}],"source":["\n","trainingdf = joineddf.select('data', 'height', 'width', 'NoFinding_mod', 'EnlargedCardiomediastinum_mod',\n","                             'Cardiomegaly_mod', 'LungOpacity_mod', 'LungLesion_mod', 'Edema_mod',\n","                             'Consolidation_mod', 'Pneumonia_mod', 'Atelectasis_mod', 'Pneumothorax_mod', \n","                             'PleuralEffusion_mod', 'PleuralOther_mod','Fracture_mod', 'SupportDevices_mod')\n","\n","\n","trainingdf.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["Making a vector column out of the 14 labels"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","\n","assembler = VectorAssembler(\n","    inputCols=[\"NoFinding_mod\", \"EnlargedCardiomediastinum_mod\", \"Cardiomegaly_mod\",\"LungOpacity_mod\", \n","              \"LungLesion_mod\", \"Edema_mod\", \"Consolidation_mod\",\"Pneumonia_mod\",\"Atelectasis_mod\",\"PleuralEffusion_mod\",\n","              \"PleuralOther_mod\",\"Fracture_mod\",\"SupportDevices_mod\"],\n","    outputCol=\"labels\")\n","\n","output = assembler.transform(joineddf)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["21/12/13 00:57:35 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","[Stage 16:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|                Path|              labels|\n","+--------------------+--------------------+\n","|CheXpert-v1.0-sma...|          (13,[],[])|\n","|CheXpert-v1.0-sma...|(13,[3,5,12],[1.0...|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[6],[1.0])|\n","|CheXpert-v1.0-sma...|     (13,[12],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[5],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[5,6],[1.0,1.0])|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[0,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|      (13,[5],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[6],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[5,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|     (13,[12],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[0,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|(13,[5,12],[1.0,1...|\n","|CheXpert-v1.0-sma...|      (13,[0],[1.0])|\n","|CheXpert-v1.0-sma...|(13,[2,3,12],[1.0...|\n","|CheXpert-v1.0-sma...|      (13,[3],[1.0])|\n","|CheXpert-v1.0-sma...|      (13,[6],[1.0])|\n","+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["output.select(col('Path'), col('labels')).show()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import col\n","from petastorm.spark import SparkDatasetConverter, make_spark_converter\n","import io\n","import numpy as np\n","import torch\n","import torchvision\n","from PIL import Image\n","from functools import partial \n","from petastorm import TransformSpec\n","from torchvision import transforms \n","# from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n","import horovod.torch as hvd\n","# from sparkdl import HorovodRunner"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["df_train, df_val = trainingdf.randomSplit([0.9, 0.1], seed=12345)\n","\n","# Make sure the number of partitions is at least the number of workers which is required for distributed training.\n","df_train = df_train.repartition(2)\n","df_val = df_val.repartition(2)"]},{"cell_type":"markdown","metadata":{},"source":["### Image data transformation"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def rawBytesToPIL(img_str, height, width):\n","    nparr = np.frombuffer(img_str, np.uint8).reshape(height, width, 1)\n","    img_np = cv2.cvtColor(nparr, cv2.COLOR_BGR2RGB)\n","    img = Image.fromarray(img_np)\n","    return img"]},{"cell_type":"markdown","metadata":{},"source":["Saving the standardized dataframe to a Parquet file"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 277 ms, sys: 50.8 ms, total: 328 ms\n","Wall time: 2min 24s\n"]}],"source":["%%time\n","df_train \\\n","    .coalesce(1) \\\n","    .write \\\n","    .mode('overwrite') \\\n","    .option('parquet.block.size', 1024*1024) \\\n","    .parquet('gs://chexpertcse6250fall2021/parquetCache4')"]},{"cell_type":"markdown","metadata":{},"source":["Create a spark converter instance for the train and validation dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Converting floating-point columns to float32\n","[Stage 34:=================================================>        (6 + 1) / 7]\r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 302 ms, sys: 110 ms, total: 411 ms\n","Wall time: 3min 6s\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["%%time\n","spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"gs://chexpertcse6250fall2021/parquetCache4\")\n","\n","converter_train = make_spark_converter(df_train)\n","converter_val = make_spark_converter(df_val)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train: 7027\n"]}],"source":["print(f\"train: {len(converter_train)}\")"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["<petastorm.spark.spark_dataset_converter.TorchDatasetContextManager at 0x7fa6d2340760>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["converter_train.make_torch_dataloader()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def metric_average(val, name):\n","    tensor = torch.tensor(val)\n","    avg_tensor = hvd.allreduce(tensor, name=name)\n","    return avg_tensor.item()\n","\n","def train_and_evaluate_hvd(lr=0.001):\n","    hvd.init()  # Initialize Horovod.\n","  \n","  # Horovod: pin GPU to local rank.\n","    if torch.cuda.is_available():\n","        torch.cuda.set_device(hvd.local_rank())\n","        device = torch.cuda.current_device()\n","    else:\n","        device = torch.device(\"cpu\")\n","  \n","    model = get_model(lr=lr)\n","    model = model.to(device)\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","  \n","  # Effective batch size in synchronous distributed training is scaled by the number of workers.\n","  # An increase in learning rate compensates for the increased batch size.\n","    optimizer = torch.optim.SGD(model.classifier[1].parameters(), lr=lr * hvd.size(), momentum=0.9)\n","  \n","  # Broadcast initial parameters so all workers start with the same parameters.\n","    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n","    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n","  \n","  # Wrap the optimizer with Horovod's DistributedOptimizer.\n","    optimizer_hvd = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n","\n","    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_hvd, step_size=7, gamma=0.1)\n","\n","    with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(is_train=True), \n","                                             cur_shard=hvd.rank(), shard_count=hvd.size(),\n","                                             batch_size=BATCH_SIZE) as train_dataloader, \\\n","       converter_val.make_torch_dataloader(transform_spec=get_transform_spec(is_train=False),\n","                                           cur_shard=hvd.rank(), shard_count=hvd.size(),\n","                                           batch_size=BATCH_SIZE) as val_dataloader:\n","    \n","        train_dataloader_iter = iter(train_dataloader)\n","        steps_per_epoch = len(converter_train) // (BATCH_SIZE * hvd.size())\n","    \n","        val_dataloader_iter = iter(val_dataloader)\n","        validation_steps = max(1, len(converter_val) // (BATCH_SIZE * hvd.size()))\n","    \n","        for epoch in range(NUM_EPOCHS):\n","            print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n","            print('-' * 10)\n","\n","            train_loss, train_acc = train_one_epoch(model, criterion, optimizer_hvd, exp_lr_scheduler, \n","                                              train_dataloader_iter, steps_per_epoch, epoch, \n","                                              device)\n","            val_loss, val_acc = evaluate(model, criterion, val_dataloader_iter, validation_steps,\n","                                   device, metric_agg_fn=metric_average)\n","\n","    return val_loss"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e74edd46504b4fd0abfa75ad596ac16f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/13.6M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["MobileNetV2(\n","  (features): Sequential(\n","    (0): ConvNormActivation(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","    (1): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (7): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (8): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (9): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (10): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (11): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (12): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (13): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (14): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (15): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (16): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (17): InvertedResidual(\n","      (conv): Sequential(\n","        (0): ConvNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): ConvNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (18): ConvNormActivation(\n","      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.2, inplace=False)\n","    (1): Linear(in_features=1280, out_features=1000, bias=True)\n","  )\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["torchvision.models.mobilenet_v2(pretrained=True)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def get_model(lr=0.001):\n","  # Load a MobileNetV2 model from torchvision\n","  model = torchvision.models.mobilenet_v2(pretrained=True)\n","  # Freeze parameters in the feature extraction layers\n","  for param in model.parameters():\n","    param.requires_grad = False\n","    \n","  # Add a new classifier layer for transfer learning\n","  num_ftrs = model.classifier[1].in_features\n","  # Parameters of newly constructed modules have requires_grad=True by default\n","  model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{},"source":["This line fails on GCP"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["# from sparkdl import HorovodRunner"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["# !pip install keras\n","# !pip install tensorflow\n","# !pip install tensorframes"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["import horovod.spark.common._namedtuple_fix\n","\n","import copy\n","import io\n","import numbers\n","import time\n","\n","from pyspark import keyword_only\n","from pyspark.ml.param.shared import Param, Params, TypeConverters\n","from pyspark.ml.util import MLWritable, MLReadable\n","from pyspark.sql import SparkSession\n","\n","from horovod.runner.common.util import codec\n","from horovod.spark.common import util\n","from horovod.spark.common.estimator import HorovodEstimator, HorovodModel\n","from horovod.spark.common.params import EstimatorParams\n","from horovod.spark.common.serialization import \\\n","    HorovodParamsWriter, HorovodParamsReader\n","from horovod.spark.torch import remote\n","from horovod.spark.torch.util import deserialize_fn, serialize_fn, \\\n","    save_into_bio"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import horovod.spark.keras as hvd\n","from horovod.spark.common.backend import SparkBackend\n","from horovod.spark.common.store import Store\n","from horovod.tensorflow.keras.callbacks import BestModelCheckpoint"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting package metadata (current_repodata.json): done\n","Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n","Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n","Collecting package metadata (repodata.json): - "]}],"source":["!conda install tensorflow"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":4}
